{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9VUSF6A00Eag"
      },
      "source": [
        "LSTM-based anomaly detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "e0Qls3AC0Eai"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.preprocessing import MinMaxScaler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tOf6lgHu0Eaj"
      },
      "source": [
        "Implementing a LSTM cell from scratch in NumPy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BimYmIOD0Eak",
        "outputId": "692b7332-bb5f-4aed-ab12-797758a11dde"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset loaded: 1,394,010 rows, 10 columns\n",
            "\n",
            "Using user 'KAS2029' with 479 days of data for verification.\n",
            "Verification sequence shape: (7, 7)\n",
            "\n",
            "Normalized data (first 3 rows):\n",
            "[[0.  0.  0.  0.  0.  0.  0.5]\n",
            " [0.  0.  0.  0.  0.  0.  1. ]\n",
            " [0.  0.  0.  0.  0.  0.  1. ]]\n"
          ]
        }
      ],
      "source": [
        "# Now we have loaded data\n",
        "df = pd.read_csv('behavioral_features.csv')\n",
        "print(f\"Dataset loaded: {df.shape[0]:,} rows, {df.shape[1]} columns\")\n",
        "\n",
        "# Getting sample user data\n",
        "sample_user = df['user'].unique()[0]\n",
        "user_data = df[df['user'] == sample_user].copy()\n",
        "print(f\"\\nUsing user '{sample_user}' with {len(user_data)} days of data for verification.\")\n",
        "\n",
        "# Define features\n",
        "FEATURE_COLS = ['logins', 'off_hour_logins', 'file_ops', 'files_copied',\n",
        "                'usb_events', 'http_uploads', 'emails_sent']\n",
        "\n",
        "# Normalize features\n",
        "features = user_data[FEATURE_COLS].values.astype(np.float32)\n",
        "scaler = MinMaxScaler()\n",
        "features_normalized = scaler.fit_transform(features)\n",
        "\n",
        "# Verification\n",
        "X_verify = features_normalized[:7]\n",
        "print(f\"Verification sequence shape: {X_verify.shape}\")\n",
        "print(f\"\\nNormalized data (first 3 rows):\\n{X_verify[:3]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NlmGqEa70Eak",
        "outputId": "4c852711-2aa2-4b75-a125-5be3be501b43"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted weights from PyTorch LSTM cell\n",
            "Weight shapes - W_ii: (4, 7), W_hi: (4, 4)\n"
          ]
        }
      ],
      "source": [
        "# Creating PyTorch LSTM Cell\n",
        "input_size = 7\n",
        "hidden_size = 4\n",
        "\n",
        "lstm_cell = nn.LSTMCell(input_size, hidden_size)\n",
        "\n",
        "# Extract weights and biases\n",
        "W_ii, W_if, W_ig, W_io = lstm_cell.weight_ih.data.chunk(4, 0)\n",
        "W_hi, W_hf, W_hg, W_ho = lstm_cell.weight_hh.data.chunk(4, 0)\n",
        "b_ii, b_if, b_ig, b_io = lstm_cell.bias_ih.data.chunk(4, 0)\n",
        "b_hi, b_hf, b_hg, b_ho = lstm_cell.bias_hh.data.chunk(4, 0)\n",
        "\n",
        "# Convert to NumPy\n",
        "weights_np = {\n",
        "    'W_ii': W_ii.numpy(), 'W_if': W_if.numpy(), 'W_ig': W_ig.numpy(), 'W_io': W_io.numpy(),\n",
        "    'W_hi': W_hi.numpy(), 'W_hf': W_hf.numpy(), 'W_hg': W_hg.numpy(), 'W_ho': W_ho.numpy(),\n",
        "    'b_ii': b_ii.numpy(), 'b_if': b_if.numpy(), 'b_ig': b_ig.numpy(), 'b_io': b_io.numpy(),\n",
        "    'b_hi': b_hi.numpy(), 'b_hf': b_hf.numpy(), 'b_hg': b_hg.numpy(), 'b_ho': b_ho.numpy()\n",
        "}\n",
        "\n",
        "print(f\"Extracted weights from PyTorch LSTM cell\")\n",
        "print(f\"Weight shapes - W_ii: {weights_np['W_ii'].shape}, W_hi: {weights_np['W_hi'].shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "3cPW5tau0Eal"
      },
      "outputs": [],
      "source": [
        "# ManualLSTM Class\n",
        "class ManualLSTM:\n",
        "    \"\"\"NumPy implementation of LSTM cell.\"\"\"\n",
        "\n",
        "    def __init__(self, weights_dict):\n",
        "        self.w = weights_dict\n",
        "\n",
        "    def sigmoid(self, x):\n",
        "        return 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n",
        "\n",
        "    def forward(self, x_seq, h_0=None, c_0=None):\n",
        "        \"\"\"Forward pass through sequence.\"\"\"\n",
        "        seq_len = x_seq.shape[0]\n",
        "        hidden_size = self.w['W_hi'].shape[0]\n",
        "\n",
        "        h = h_0 if h_0 is not None else np.zeros(hidden_size, dtype=np.float32)\n",
        "        c = c_0 if c_0 is not None else np.zeros(hidden_size, dtype=np.float32)\n",
        "\n",
        "        outputs = []\n",
        "        for t in range(seq_len):\n",
        "            x_t = x_seq[t]\n",
        "\n",
        "            # Input gate\n",
        "            i_t = self.sigmoid(x_t @ self.w['W_ii'].T + self.w['b_ii'] +\n",
        "                              h @ self.w['W_hi'].T + self.w['b_hi'])\n",
        "            # Forget gate\n",
        "            f_t = self.sigmoid(x_t @ self.w['W_if'].T + self.w['b_if'] +\n",
        "                              h @ self.w['W_hf'].T + self.w['b_hf'])\n",
        "            # Cell gate\n",
        "            g_t = np.tanh(x_t @ self.w['W_ig'].T + self.w['b_ig'] +\n",
        "                         h @ self.w['W_hg'].T + self.w['b_hg'])\n",
        "            # Output gate\n",
        "            o_t = self.sigmoid(x_t @ self.w['W_io'].T + self.w['b_io'] +\n",
        "                              h @ self.w['W_ho'].T + self.w['b_ho'])\n",
        "\n",
        "            # Update cell state and hidden state\n",
        "            c = f_t * c + i_t * g_t\n",
        "            h = o_t * np.tanh(c)\n",
        "            outputs.append(h.copy())\n",
        "\n",
        "        return np.array(outputs), h, c"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ManualLSTM class is now defined\n",
        "\n"
      ],
      "metadata": {
        "id": "o39gLHIYjkqu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9d169EoI0Eal",
        "outputId": "f377dea2-36b2-4872-89f1-07e1b7bdf42c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch output shape: (7, 4)\n",
            "Manual output shape: (7, 4)\n",
            "\n",
            " np.allclose(manual_output, torch_output) = True\n",
            "\n",
            " NumPy LSTM matches PyTorch exactly\n"
          ]
        }
      ],
      "source": [
        "# Verify Manual vs PyTorch Output\n",
        "# PyTorch forward pass\n",
        "X_torch = torch.tensor(X_verify, dtype=torch.float32)\n",
        "h_torch = torch.zeros(hidden_size)\n",
        "c_torch = torch.zeros(hidden_size)\n",
        "\n",
        "torch_outputs = []\n",
        "for t in range(X_torch.shape[0]):\n",
        "    h_torch, c_torch = lstm_cell(X_torch[t:t+1], (h_torch.unsqueeze(0), c_torch.unsqueeze(0)))\n",
        "    h_torch = h_torch.squeeze(0)\n",
        "    c_torch = c_torch.squeeze(0)\n",
        "    torch_outputs.append(h_torch.detach().numpy().copy())\n",
        "torch_outputs = np.array(torch_outputs)\n",
        "\n",
        "# Manual NumPy forward pass\n",
        "manual_lstm = ManualLSTM(weights_np)\n",
        "manual_outputs, _, _ = manual_lstm.forward(X_verify)\n",
        "\n",
        "# Verification\n",
        "match = np.allclose(manual_outputs, torch_outputs, atol=1e-6)\n",
        "max_diff = np.max(np.abs(manual_outputs - torch_outputs))\n",
        "\n",
        "print(f\"PyTorch output shape: {torch_outputs.shape}\")\n",
        "print(f\"Manual output shape: {manual_outputs.shape}\")\n",
        "print(f\"\\n np.allclose(manual_output, torch_output) = {match}\")\n",
        "print(\"\\n NumPy LSTM matches PyTorch exactly\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1D4ne9ch0Eal"
      },
      "source": [
        "Data Pipeline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X6wrxbBk0Eam",
        "outputId": "d1607aa8-1450-4e1d-87d2-1b38ebb60ec5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total unique users: 4000\n",
            "Train users: 3200 (80.0%)\n",
            "Test users: 800 (20.0%)\n",
            "\n",
            "Train samples: 1,114,121\n",
            "Test samples: 279,889\n"
          ]
        }
      ],
      "source": [
        "# Spilt\n",
        "all_users = df['user'].unique()\n",
        "np.random.shuffle(all_users)\n",
        "\n",
        "split_idx = int(len(all_users) * 0.8)\n",
        "train_users = all_users[:split_idx]\n",
        "test_users = all_users[split_idx:]\n",
        "\n",
        "train_df = df[df['user'].isin(train_users)].copy()\n",
        "test_df = df[df['user'].isin(test_users)].copy()\n",
        "\n",
        "print(f\"Total unique users: {len(all_users)}\")\n",
        "print(f\"Train users: {len(train_users)} ({len(train_users)/len(all_users)*100:.1f}%)\")\n",
        "print(f\"Test users: {len(test_users)} ({len(test_users)/len(all_users)*100:.1f}%)\")\n",
        "print(f\"\\nTrain samples: {len(train_df):,}\")\n",
        "print(f\"Test samples: {len(test_df):,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N9vHTx8J0Eam",
        "outputId": "ca32261f-b352-499a-e08a-e9ddca74f90f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scaler fitted on training data only\n",
            "Train scaled shape: (1114121, 7)\n",
            "Test scaled shape: (279889, 7)\n"
          ]
        }
      ],
      "source": [
        "# Fit on train users only\n",
        "scaler = MinMaxScaler()\n",
        "scaler.fit(train_df[FEATURE_COLS])\n",
        "\n",
        "train_scaled = scaler.transform(train_df[FEATURE_COLS]).astype(np.float32)\n",
        "test_scaled = scaler.transform(test_df[FEATURE_COLS]).astype(np.float32)\n",
        "\n",
        "print(f\"Scaler fitted on training data only\")\n",
        "print(f\"Train scaled shape: {train_scaled.shape}\")\n",
        "print(f\"Test scaled shape: {test_scaled.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZWORkQVj0Eam",
        "outputId": "e19097ac-70bb-4a47-ba90-c36300c2bb62"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training sequences: X=(1091721, 7, 7), y=(1091721, 7)\n",
            "Test sequences: X=(274289, 7, 7), y=(274289, 7)\n"
          ]
        }
      ],
      "source": [
        "# Windowing\n",
        "WINDOW_SIZE = 7\n",
        "\n",
        "def create_sequences_fast(data_df, scaled_data, window_size=7):\n",
        "    \"\"\"Create sequences per user using sliding window.\"\"\"\n",
        "    X_list, y_list = [], []\n",
        "    users = data_df['user'].unique()\n",
        "\n",
        "    start_idx = 0\n",
        "    for user in users:\n",
        "        user_len = len(data_df[data_df['user'] == user])\n",
        "        user_data = scaled_data[start_idx:start_idx + user_len]\n",
        "\n",
        "        if len(user_data) > window_size:\n",
        "\n",
        "            windows = np.lib.stride_tricks.sliding_window_view(user_data, (window_size, user_data.shape[1]))\n",
        "            windows = windows.squeeze(1)\n",
        "\n",
        "            X_user = windows[:-1]  # All windows except last\n",
        "            y_user = user_data[window_size:]  # Next-day targets\n",
        "\n",
        "            X_list.append(X_user)\n",
        "            y_list.append(y_user)\n",
        "\n",
        "        start_idx += user_len\n",
        "\n",
        "    return np.concatenate(X_list), np.concatenate(y_list)\n",
        "\n",
        "# Sort data by user\n",
        "train_df_sorted = train_df.sort_values(['user', 'day']).reset_index(drop=True)\n",
        "test_df_sorted = test_df.sort_values(['user', 'day']).reset_index(drop=True)\n",
        "\n",
        "# Re-scale after sorting\n",
        "train_scaled = scaler.transform(train_df_sorted[FEATURE_COLS]).astype(np.float32)\n",
        "test_scaled = scaler.transform(test_df_sorted[FEATURE_COLS]).astype(np.float32)\n",
        "\n",
        "# Create sequences\n",
        "X_train, y_train = create_sequences_fast(train_df_sorted, train_scaled, WINDOW_SIZE)\n",
        "X_test, y_test = create_sequences_fast(test_df_sorted, test_scaled, WINDOW_SIZE)\n",
        "\n",
        "print(f\"Training sequences: X={X_train.shape}, y={y_train.shape}\")\n",
        "print(f\"Test sequences: X={X_test.shape}, y={y_test.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f7xGlkxJ0Eam",
        "outputId": "b520ae68-ec79-4bb7-ca65-446dcfeb5aba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "DataLoaders created:\n",
            "  Train: 427 batches (873,376 samples)\n",
            "  Val: 107 batches (218,345 samples)\n",
            "  Test: 134 batches (274,289 samples)\n",
            "  Batch size: 2048\n"
          ]
        }
      ],
      "source": [
        "# DataLoader\n",
        "BATCH_SIZE = 2048\n",
        "\n",
        "# Split training into train/val (80/20)\n",
        "val_split = int(len(X_train) * 0.8)\n",
        "X_train_final, X_val = X_train[:val_split], X_train[val_split:]\n",
        "y_train_final, y_val = y_train[:val_split], y_train[val_split:]\n",
        "\n",
        "# Create DataLoaders\n",
        "train_dataset = TensorDataset(torch.tensor(X_train_final), torch.tensor(y_train_final))\n",
        "val_dataset = TensorDataset(torch.tensor(X_val), torch.tensor(y_val))\n",
        "test_dataset = TensorDataset(torch.tensor(X_test), torch.tensor(y_test))\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
        "\n",
        "print(f\"\\nDataLoaders created:\")\n",
        "print(f\"  Train: {len(train_loader)} batches ({len(X_train_final):,} samples)\")\n",
        "print(f\"  Val: {len(val_loader)} batches ({len(X_val):,} samples)\")\n",
        "print(f\"  Test: {len(test_loader)} batches ({len(X_test):,} samples)\")\n",
        "print(f\"  Batch size: {BATCH_SIZE}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q4CtYBUn0Eam"
      },
      "source": [
        "Now we try to find the optimal architecture."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "5951e_i20Ean"
      },
      "outputs": [],
      "source": [
        "# Define  Sequence Model\n",
        "class SequenceModel(nn.Module):\n",
        "    \"\"\" RNN model supporting LSTM and GRU.\"\"\"\n",
        "\n",
        "    def __init__(self, input_size, hidden_size, num_layers, dropout, rnn_type='LSTM'):\n",
        "        super().__init__()\n",
        "        self.rnn_type = rnn_type\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        # Dynamically select RNN type\n",
        "        rnn_class = nn.LSTM if rnn_type == 'LSTM' else nn.GRU\n",
        "        self.rnn = rnn_class(\n",
        "            input_size=input_size,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            dropout=dropout if num_layers > 1 else 0,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.fc = nn.Linear(hidden_size, input_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out, _ = self.rnn(x)\n",
        "        out = self.fc(out[:, -1, :])  # Last timestep\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SequenceModel class is now defined\n",
        "\n"
      ],
      "metadata": {
        "id": "CH4lZVESjRha"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jqiHD9lb0Ean",
        "outputId": "353407f9-7eaa-4264-abd5-e9948802ce83"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Architectures Defined:\n",
            "  Baseline_LSTM: LSTM, H=64, L=1, D=0.0\n",
            "  GRU: GRU, H=64, L=1, D=0.0\n",
            "  Deep_stack_LSTM: LSTM, H=64, L=2, D=0.2\n",
            "  Lightweight_LSTM: LSTM, H=16, L=1, D=0.0\n",
            "  Large_LSTM: LSTM, H=128, L=1, D=0.3\n"
          ]
        }
      ],
      "source": [
        "# Architecture Configurations\n",
        "CONFIGS = {\n",
        "    'Baseline_LSTM': {\n",
        "        'rnn_type': 'LSTM', 'hidden_size': 64, 'num_layers': 1, 'dropout': 0.0,\n",
        "    },\n",
        "    'GRU': {\n",
        "        'rnn_type': 'GRU', 'hidden_size': 64, 'num_layers': 1, 'dropout': 0.0,\n",
        "    },\n",
        "    'Deep_stack_LSTM': {\n",
        "        'rnn_type': 'LSTM', 'hidden_size': 64, 'num_layers': 2, 'dropout': 0.2,\n",
        "    },\n",
        "    'Lightweight_LSTM': {\n",
        "        'rnn_type': 'LSTM', 'hidden_size': 16, 'num_layers': 1, 'dropout': 0.0,\n",
        "    },\n",
        "    'Large_LSTM': {\n",
        "        'rnn_type': 'LSTM', 'hidden_size': 128, 'num_layers': 1, 'dropout': 0.3,\n",
        "\n",
        "    }\n",
        "}\n",
        "\n",
        "print(\" Architectures Defined:\")\n",
        "for name, cfg in CONFIGS.items():\n",
        "    print(f\"  {name}: {cfg['rnn_type']}, H={cfg['hidden_size']}, L={cfg['num_layers']}, D={cfg['dropout']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "6e_0hMER0Ean"
      },
      "outputs": [],
      "source": [
        "# Training Function\n",
        "def train_model(config_name, config, train_loader, val_loader, epochs=3):\n",
        "    \"\"\"Train a model and return results.\"\"\"\n",
        "    print(f\"Training: {config_name}\")\n",
        "    model = SequenceModel(\n",
        "        input_size=7,\n",
        "        hidden_size=config['hidden_size'],\n",
        "        num_layers=config['num_layers'],\n",
        "        dropout=config['dropout'],\n",
        "        rnn_type=config['rnn_type']\n",
        "    )\n",
        "\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    train_losses, val_losses = [], []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Training\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        for X_batch, y_batch in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(X_batch)\n",
        "            loss = criterion(outputs, y_batch)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item()\n",
        "        train_loss /= len(train_loader)\n",
        "        train_losses.append(train_loss)\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for X_batch, y_batch in val_loader:\n",
        "                outputs = model(X_batch)\n",
        "                loss = criterion(outputs, y_batch)\n",
        "                val_loss += loss.item()\n",
        "        val_loss /= len(val_loader)\n",
        "        val_losses.append(val_loss)\n",
        "\n",
        "        print(f\"   Epoch {epoch+1}/{epochs} - Train Loss: {train_loss:.6f}, Val Loss: {val_loss:.6f}\")\n",
        "\n",
        "    final_val_loss = val_losses[-1]\n",
        "    print(f\" Final Validation Loss: {final_val_loss:.6f}\")\n",
        "\n",
        "    return {\n",
        "        'model_state': model.state_dict(),\n",
        "        'final_val_loss': final_val_loss,\n",
        "        'train_losses': train_losses,\n",
        "        'val_losses': val_losses\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dDm2Jhoc0Ean",
        "outputId": "9f551ac6-26bc-426a-e0cd-c1b87013051e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training: Baseline_LSTM\n",
            "   Epoch 1/3 - Train Loss: 0.002767, Val Loss: 0.001408\n",
            "   Epoch 2/3 - Train Loss: 0.001366, Val Loss: 0.001379\n",
            "   Epoch 3/3 - Train Loss: 0.001354, Val Loss: 0.001371\n",
            " Final Validation Loss: 0.001371\n",
            "Training: GRU\n",
            "   Epoch 1/3 - Train Loss: 0.002600, Val Loss: 0.001407\n",
            "   Epoch 2/3 - Train Loss: 0.001366, Val Loss: 0.001369\n",
            "   Epoch 3/3 - Train Loss: 0.001353, Val Loss: 0.001367\n",
            " Final Validation Loss: 0.001367\n",
            "Training: Deep_stack_LSTM\n",
            "   Epoch 1/3 - Train Loss: 0.002846, Val Loss: 0.001438\n",
            "   Epoch 2/3 - Train Loss: 0.001417, Val Loss: 0.001380\n",
            "   Epoch 3/3 - Train Loss: 0.001372, Val Loss: 0.001362\n",
            " Final Validation Loss: 0.001362\n",
            "Training: Lightweight_LSTM\n",
            "   Epoch 1/3 - Train Loss: 0.004892, Val Loss: 0.001632\n",
            "   Epoch 2/3 - Train Loss: 0.001469, Val Loss: 0.001439\n",
            "   Epoch 3/3 - Train Loss: 0.001395, Val Loss: 0.001403\n",
            " Final Validation Loss: 0.001403\n",
            "Training: Large_LSTM\n",
            "   Epoch 1/3 - Train Loss: 0.002288, Val Loss: 0.001390\n",
            "   Epoch 2/3 - Train Loss: 0.001357, Val Loss: 0.001364\n",
            "   Epoch 3/3 - Train Loss: 0.001345, Val Loss: 0.001355\n",
            " Final Validation Loss: 0.001355\n",
            "Results: \n",
            "  Large_LSTM: 0.001355\n",
            "  Deep_stack_LSTM: 0.001362\n",
            "  GRU: 0.001367\n",
            "  Baseline_LSTM: 0.001371\n",
            "  Lightweight_LSTM: 0.001403\n"
          ]
        }
      ],
      "source": [
        "\n",
        "EPOCHS = 3\n",
        "results = {}\n",
        "\n",
        "for config_name, config in CONFIGS.items():\n",
        "    result = train_model(config_name, config, train_loader, val_loader, epochs=EPOCHS)\n",
        "    results[config_name] = result['final_val_loss']\n",
        "    # Storing full results\n",
        "    CONFIGS[config_name]['model_state'] = result['model_state']\n",
        "\n",
        "print(\"Results: \")\n",
        "for name, loss in sorted(results.items(), key=lambda x: x[1]):\n",
        "    print(f\"  {name}: {loss:.6f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DAxKrJ6p0Eao"
      },
      "source": [
        "Model selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lv0eWxYU0Eao",
        "outputId": "d50f09d9-099d-44a2-d262-4949c8d70491"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Large_LSTM configuration is the best model.\n",
            "Final Validation Loss is: 0.001355\n"
          ]
        }
      ],
      "source": [
        "# Selecting Best model\n",
        "best_config_name = min(results, key=results.get)\n",
        "best_val_loss = results[best_config_name]\n",
        "\n",
        "print(f\"\\n{best_config_name} configuration is the best model.\")\n",
        "print(f\"Final Validation Loss is: {best_val_loss:.6f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wWa7n1440Eao",
        "outputId": "70b5ee15-062e-46b1-c408-203fd05020a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Re-instantiated Large_LSTM model.\n",
            "   Type: LSTM\n",
            "   Hidden Size: 128\n",
            "   Layers: 1\n",
            "   Dropout: 0.3\n"
          ]
        }
      ],
      "source": [
        "best_cfg = CONFIGS[best_config_name]\n",
        "\n",
        "best_model = SequenceModel(\n",
        "    input_size=7,\n",
        "    hidden_size=best_cfg['hidden_size'],\n",
        "    num_layers=best_cfg['num_layers'],\n",
        "    dropout=best_cfg['dropout'],\n",
        "    rnn_type=best_cfg['rnn_type']\n",
        ")\n",
        "\n",
        "best_model.load_state_dict(best_cfg['model_state'])\n",
        "best_model.eval()\n",
        "\n",
        "print(f\"\\nRe-instantiated {best_config_name} model.\")\n",
        "print(f\"   Type: {best_cfg['rnn_type']}\")\n",
        "print(f\"   Hidden Size: {best_cfg['hidden_size']}\")\n",
        "print(f\"   Layers: {best_cfg['num_layers']}\")\n",
        "print(f\"   Dropout: {best_cfg['dropout']}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EUaSdP520Eao",
        "outputId": "5e53b063-b276-41fa-ad3b-6a139aee3b59"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Threshold Computation\n",
            "Training Errors - Mean: 0.001339, Std: 0.004691\n",
            "95th Percentile Threshold: 0.006497\n"
          ]
        }
      ],
      "source": [
        "# Compute Threshold\n",
        "def compute_errors(model, loader):\n",
        "    \"\"\"Compute reconstruction errors for a dataset.\"\"\"\n",
        "    model.eval()\n",
        "    errors = []\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in loader:\n",
        "            predictions = model(X_batch)\n",
        "            batch_errors = torch.mean((predictions - y_batch) ** 2, dim=1)\n",
        "            errors.extend(batch_errors.numpy())\n",
        "    return np.array(errors)\n",
        "\n",
        "# Compute errors on full training set\n",
        "full_train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
        "train_errors = compute_errors(best_model, full_train_loader)\n",
        "\n",
        "# Threshold at 95th percentile\n",
        "threshold = np.percentile(train_errors, 95)\n",
        "\n",
        "print(f\"\\n Threshold Computation\")\n",
        "print(f\"Training Errors - Mean: {np.mean(train_errors):.6f}, Std: {np.std(train_errors):.6f}\")\n",
        "print(f\"95th Percentile Threshold: {threshold:.6f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dyDXg-Ov0Eao",
        "outputId": "1281ca84-87d0-4ee6-c92e-f013d466e49a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Errors - Mean: 0.001167, Std: 0.004240\n",
            "Anomalies Detected: 12,129 / 274,289 (4.42%)\n"
          ]
        }
      ],
      "source": [
        "# Evaluate on Test Set\n",
        "test_errors = compute_errors(best_model, test_loader)\n",
        "anomalies = test_errors > threshold\n",
        "\n",
        "print(f\"Test Errors - Mean: {np.mean(test_errors):.6f}, Std: {np.std(test_errors):.6f}\")\n",
        "print(f\"Anomalies Detected: {np.sum(anomalies):,} / {len(test_errors):,} ({np.mean(anomalies)*100:.2f}%)\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}